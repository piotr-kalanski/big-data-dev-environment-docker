version: '2'

services:
  # this is our kafka cluster.
  kafka-cluster:
    image: landoop/fast-data-dev:latest
    environment:
      ADV_HOST: 127.0.0.1
      RUNTESTS: 0                 # Disable Running tests so the cluster starts faster
    ports:
      - 2181:2181                 # Zookeeper
      - 3030:3030                 # Landoop UI
      - 8081-8083:8081-8083       # REST Proxy, Schema Registry, Kafka Connect ports
      - 9581-9585:9581-9585       # JMX Ports
      - 9092:9092                 # Kafka Broker
    volumes:
      - ./kafka-connect-jdbc:/usr/share/java/kafka-connect-jdbc # bind mount for Kafka additional JDBC drivers

  # This configuration allows you to start Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.1.1
    environment:
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - 9200:9200
      - 9300:9300

  # This configuration allows you to start Kibana
  kibana:
    image: docker.elastic.co/kibana/kibana:6.1.1
    ports:
      - 5601:5601
    depends_on:
      - elasticsearch

  # This configuration allows you to start Postgres
  postgres:
    image: postgres:9.5-alpine
    environment:
      POSTGRES_USER: postgres     # define credentials
      POSTGRES_PASSWORD: postgres # define credentials
      POSTGRES_DB: postgres       # define database
    volumes:
      - ./postgres_users:/docker-entrypoint-initdb.d # create additional users
    ports:
      - 5432:5432                 # Postgres port

  # This configuration allows you to start MS SQL DB
  mssql:
    image: microsoft/mssql-server-linux:latest
    environment:
      ACCEPT_EULA: Y
      SA_PASSWORD: P4sw0rd123
    ports:
      - 1433:1433                 # MSSQL port

  # This configuration allows you to start My SQL DB
  mysql:
    image: mysql
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: mysql  # password for user root
    ports:
      - 3306:3306

  # This configuration allows you to start airflow
  airflow:
    image: puckel/docker-airflow:latest
    restart: always
    depends_on:
        - postgres
    volumes:
        - ./dags:/usr/local/airflow/dags # bind mount for airflow dags
    ports:
        - 8080:8080               # airflow port
    command: webserver

  # This configuration allows you to start Hadoop name node
  namenode:
    image: uhopper/hadoop-namenode:2.8.1
    container_name: namenode
    environment:
      - CLUSTER_NAME=hadoop_dev
      - HDFS_CONF_dfs_replication=1 # change replication to 1, because only 1 data node
      - HDFS_CONF_dfs_permissions_enabled=false # permission checking is turned off
    ports:
      - 8020:8020
      - 50070:50070

  # This configuration allows you to start Hadoop data node
  datanode1:
    image: uhopper/hadoop-datanode:2.8.1
    container_name: datanode1
    hostname: datanode1
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    ports:
      - 50010:50010
      - 50020:50020
      - 50075:50075

  # This configuration allows you to start YARN resource manager
  resourcemanager:
    image: uhopper/hadoop-resourcemanager:2.8.1
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - YARN_CONF_yarn_log___aggregation___enable=true
    ports:
      - 8088:8088

  # This configuration allows you to start YARN node manager
  nodemanager1:
    image: uhopper/hadoop-nodemanager:2.8.1
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
      - YARN_CONF_yarn_log___aggregation___enable=true
      - YARN_CONF_yarn_nodemanager_remote___app___log___dir=/app-logs

  # This configuration allows you to start Hadoop edge node with Spark
  spark:
    image: uhopper/hadoop-spark:2.1.2_2.8.1
    container_name: spark
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
    command: tail -f /var/log/dmesg
    ports:
      - 18080:18080
      - 4040:4040
    volumes:
      - ./spark-jobs:/opt/transformations     # bind mount for Spark jar jobs

  # This configuration allows you to start local S3 based on fakes3 (https://github.com/jubos/fake-s3)
  s3:
    image: lphoward/fake-s3
    ports:
      - 4569:4569

  dynamodb:
    image: cnadiminti/dynamodb-local
    container_name: dynamodb
    ports:
      - 8000:8000
